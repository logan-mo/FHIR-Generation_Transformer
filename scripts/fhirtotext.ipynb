{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import spacy\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(string):\n",
    "    # Initialize an empty dictionary to store the extracted features\n",
    "    features = {}\n",
    "\n",
    "    # Extract resourceType\n",
    "    resource_type_match = re.search(r'\\[resourceType\\]\\s(.*?)\\s', string)\n",
    "    if resource_type_match:\n",
    "        resource_type = resource_type_match.group(1)\n",
    "        features['resourceType'] = resource_type\n",
    "\n",
    "    # Extract id\n",
    "    id_match = re.search(r'\\[id\\]\\s(.*?)\\s', string)\n",
    "    if id_match:\n",
    "        id_value = id_match.group(1)\n",
    "        features['id'] = id_value\n",
    "    \n",
    "    references = re.findall(r'\\[reference\\]\\s(.*?)\\s', string)\n",
    "    if references:\n",
    "        features['references'] = references\n",
    "    \n",
    "    \n",
    "    matches = re.findall(r'\\[system\\]\\s(.*?)\\s.*?\\[code\\]\\s(.*?)\\s', string)\n",
    "    # 'matches' is a list of tuples where the first item is the system and the second is the code\n",
    "    # transform it into a list of dictionaries for easier use\n",
    "    codings = [system + '|' + code for system, code in matches]\n",
    "    if codings:\n",
    "        features['codings'] = codings\n",
    "\n",
    "    # Extract date/time features\n",
    "    date_time_values = re.findall(r'\\[.*?]\\[.*?]\\[.*?DateTime\\]\\s(.*?)\\s', string)\n",
    "    for i, date_time in enumerate(date_time_values):\n",
    "        feature_name = f'dateTime_{i+1}'\n",
    "        date_time_obj = datetime.fromisoformat(date_time)\n",
    "        features[feature_name + '_year'] = date_time_obj.year\n",
    "        features[feature_name + '_month'] = date_time_obj.month\n",
    "        features[feature_name + '_day'] = date_time_obj.day\n",
    "        features[feature_name + '_hour'] = date_time_obj.hour\n",
    "\n",
    "    # Process text features with spaCy\n",
    "    text_values = re.findall(r'\\[.*?]\\[.*?]\\[.*?text\\]\\s(.*?)\\s', string)\n",
    "    for i, text in enumerate(text_values):\n",
    "        feature_name = f'text_{i+1}'\n",
    "        # Apply spaCy for tokenization and linguistic feature extraction\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.text for token in doc]\n",
    "        features[feature_name + '_tokens'] = tokens\n",
    "        features[feature_name + '_num_tokens'] = len(tokens)\n",
    "\n",
    "        # Extract relevant linguistic features\n",
    "        # Example: POS tags, dependency labels, named entities\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        features[feature_name + '_pos_tags'] = pos_tags\n",
    "        dependency_labels = [token.dep_ for token in doc]\n",
    "        features[feature_name + '_dependency_labels'] = dependency_labels\n",
    "        named_entities = [ent.label_ for ent in doc.ents]\n",
    "        features[feature_name + '_named_entities'] = named_entities\n",
    "\n",
    "    # Categorical encoding - One-Hot Encoding\n",
    "    categorical_labels = re.findall(r'\\[.*?]\\[.*?]\\[.*?]\\[(.*?)\\]', string)\n",
    "    for label in categorical_labels:\n",
    "        # One-hot encode the categorical variables\n",
    "        one_hot_encoding = pd.get_dummies([label], prefix=label)\n",
    "        features.update(one_hot_encoding.iloc[0].to_dict())\n",
    "\n",
    "        # Return the extracted features as a DataFrame\n",
    "        \n",
    "    # Extract other labels and perform encoding\n",
    "    labels = re.findall(r'\\[(.*?)\\]', string)\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Perform one-hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_labels = onehot_encoder.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "\n",
    "    # Convert the one-hot encoded labels to a DataFrame\n",
    "    label_columns = [f'label_{i}' for i in range(onehot_labels.shape[1])]\n",
    "    labels_df = pd.DataFrame(onehot_labels, columns=label_columns)\n",
    "\n",
    "    # Merge the label DataFrame with the features dictionary\n",
    "    features.update(labels_df.to_dict(orient='records')[0])\n",
    "\n",
    "    # Return the extracted features as a DataFrame\n",
    "    features_df = pd.DataFrame([features])\n",
    "\n",
    "    return features_df \n",
    "\n",
    "def dict_to_string(d, parent_key=''):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}[{k}]\" if parent_key else f\"[{k}]\"\n",
    "        if isinstance(v, dict):\n",
    "            items.append(dict_to_string(v, new_key))\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    items.append(dict_to_string(item, f\"{new_key}[{i}]\"))\n",
    "                else:\n",
    "                    items.append(f\"{new_key}[{i}] {item}\")\n",
    "        else:\n",
    "            items.append(f\"{new_key} {v}\")\n",
    "    return \" \".join(items).replace('\\n', '')\n",
    "\n",
    "# Define the directory\n",
    "directory = 'data'\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_references(string):\n",
    "    references = re.findall(r'\\[reference\\]\\s(.*?)\\s', string)\n",
    "    return references\n",
    "\n",
    "def extract_resource_type(string):\n",
    "    resource_type_match = re.search(r'\\[resourceType\\]\\s(.*?)\\s', string)\n",
    "    if resource_type_match:\n",
    "        resource_type = resource_type_match.group(1)\n",
    "        return resource_type\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_id(string):\n",
    "    id_match = re.search(r'\\[id\\]\\s(.*?)\\s', string)\n",
    "    if id_match:\n",
    "        resource_id = id_match.group(1)\n",
    "        return resource_id\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_codings(string):\n",
    "    matches = re.findall(r'\\[system\\]\\s(.*?)\\s.*?\\[code\\]\\s(.*?)\\s', string)\n",
    "    # 'matches' is a list of tuples where the first item is the system and the second is the code\n",
    "    # transform it into a list of dictionaries for easier use\n",
    "    codings = [system + '|' + code for system, code in matches]\n",
    "    return codings\n",
    "\n",
    "def extract_date_time_values(string):\n",
    "    date_time_labels = re.findall(r'\\[(.*?)\\]', string)\n",
    "    date_time_values = []\n",
    "\n",
    "    for label in date_time_labels:\n",
    "        if 'Date' in label or 'Time' in label:\n",
    "            value_match = re.search(rf'\\[{label}\\]\\s(.*?)\\s', string)\n",
    "            if value_match:\n",
    "                date_time_values.append(value_match.group(1))\n",
    "    \n",
    "    return date_time_values\n",
    "\n",
    "def extract_nlp_labels(string):\n",
    "    nlp_labels = re.findall(r'\\[text\\]\\s(.*?)\\s', string)\n",
    "    return nlp_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the output file\n",
    "with open('output/csv/allergyintolerance.csv', 'w', encoding='utf-8', newline='') as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow([\"resourceType\", \"data\"])\n",
    "\n",
    "    directory = \"data\"\n",
    "\n",
    "    # Loop over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        # Only process .ndjson files\n",
    "        if filename.startswith('Allergy'):\n",
    "\n",
    "            # Open the ndjson file\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as in_file:\n",
    "                features_df = None\n",
    "                # Process each line in the file\n",
    "                for line in in_file:\n",
    "                    resource = json.loads(line)\n",
    "                    resource_type = resource.get(\"resourceType\", \"Unknown\")\n",
    "                    resource_data = dict_to_string(resource)\n",
    "                    # Example usage\n",
    "                    string = resource_data\n",
    "\n",
    "                    resource_type = extract_resource_type(string)\n",
    "                    resource_id = extract_id(string)\n",
    "                    codings = extract_codings(string)\n",
    "                    date_time_values = extract_date_time_values(string)\n",
    "                    nlp_labels = extract_nlp_labels(string)\n",
    "                    references = extract_references(string)\n",
    "                    #print(string)\n",
    "                    #print(f\"Resource Type: {resource_type}\")\n",
    "                    #print(f\"Resource ID: {resource_id}\")\n",
    "                    #print(f\"Codings: {codings}\")\n",
    "                    #print(f\"Date/Time Values: {date_time_values}\")\n",
    "                    #rint(f\"NLP Labels: {nlp_labels}\")\n",
    "                    #print(f\"References: {references}\")\n",
    "                    # Perform feature engineering on the string\n",
    "                    if features_df is None:\n",
    "                        features_df = feature_engineering(string)\n",
    "                    else:\n",
    "                        features_df = pd.concat([features_df, feature_engineering(string)], axis=0)\n",
    "                    # Print the extracted features\n",
    "                    #print(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv('example.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
