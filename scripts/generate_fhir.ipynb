{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_data = []\n",
    "with open('data/chunk_1.csv', 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        tokenized_data.append(tokenizer(line))\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_data)\n",
    "\n",
    "# Convert tokens to indices\n",
    "sequences = []\n",
    "for token_sequence in tokenized_data:\n",
    "    sequences.append([vocab[token] for token in token_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 sequences as indices\n",
    "for sequence in sequences[:5]:\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 sequences as tokens\n",
    "for sequence in sequences[:5]:\n",
    "    print([vocab.get_itos()[index] for index in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Set the hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "num_epochs = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (sequences, targets) = zip(*batch)\n",
    "    sequences_pad = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    targets_pad = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return sequences_pad, targets_pad\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Load model and optimizer states from a PRETRAINED MODEL\n",
    "\n",
    "if os.path.exists('saved_models/checkpoint2.pth'):\n",
    "    checkpoint = torch.load('saved_models/checkpoint2.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Convert the sequences to tensors and move them to the device\n",
    "sequences = [torch.tensor(sequence).to(device) for sequence in sequences]\n",
    "\n",
    "# Prepare the data for training\n",
    "inputs = [sequence[:-1] for sequence in sequences]\n",
    "targets = [sequence[1:] for sequence in sequences]\n",
    "\n",
    "# Combine the inputs and targets into a single dataset\n",
    "dataset = list(zip(inputs, targets))\n",
    "\n",
    "# Split the dataset into a training set and a validation set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# Set the number of training epochs and the learning rate\n",
    "# Set sequence length\n",
    "sequence_length = max(len(sequence) for sequence, _ in train_dataset)\n",
    "# Set sequence length\n",
    "#sequence_length = 100\n",
    "# Pad or truncate all sequences to this length  FOR USE ONLY WITH GAN\n",
    "#train_dataset = [sequence[:sequence_length] for sequence in train_dataset]\n",
    "\n",
    "\n",
    "# Create data loaders for the training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "# Inside the training loop...\n",
    "    for i, (sequence, target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(sequence)\n",
    "\n",
    "        # Reshape the output and target tensors\n",
    "        output = output.view(-1, output.shape[-1])  # shape: (sequence_length * batch_size, num_classes)\n",
    "        target = target.view(-1)  # shape: (sequence_length * batch_size,)\n",
    "\n",
    "        # for sequence1, sequence2 in zip(output.int(), target.int()):\n",
    "        #     print(' '.join([vocab.get_itos()[index] for index in sequence]))\n",
    "        #     print(' '.join([vocab.get_itos()[index] for index in sequence2]))\n",
    "        #     print(\"-----------------------------------------------\")\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Step: \" + str(i) + \" loss: \" + str(loss.item()))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}')\n",
    "\n",
    "    # Validate the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for sequence, target in val_loader:\n",
    "            output = model(sequence)\n",
    "\n",
    "            # Reshape the output and target tensors\n",
    "            output = output.view(-1, output.shape[-1])  # shape: (sequence_length * batch_size, num_classes)\n",
    "            target = target.view(-1)  # shape: (sequence_length * batch_size,)\n",
    "\n",
    "            loss = loss_function(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {(val_loss / len(val_loader)).item()}')\n",
    "\n",
    "#The training loop should execute below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.forward(torch.tensor([vocab['[resourcetype]']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(dim=1)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_itos()[pred.argmax(dim=1)[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence1, sequence2 in zip(output.int(), target.int()):\n",
    "    print(' '.join([vocab.get_itos()[index] for index in sequence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saved_model_dir = \"./saved_models/\"\n",
    "\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose 'model' is your RNNModel\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'saved_models/checkpoint2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in train_dataset[0]:\n",
    "    print(' '.join([vocab.get_itos()[index] for index in sequence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS THE SECTION WITH AN EXPERIMENTAL GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator (similar to the original RNN model)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Define the discriminator (a binary classifier)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of training epochs and the learning rate\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the generator and discriminator, and move them to the device\n",
    "generator = Generator(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "discriminator = Discriminator(embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "# Set the loss function and the optimizers\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    for i, (real_sequences, _) in enumerate(train_loader):\n",
    "        # Move the real sequences to the device\n",
    "        real_sequences = real_sequences.to(device)\n",
    "\n",
    "        # Create random noise sequences as input for the generator\n",
    "        noise_sequences = torch.randint(vocab_size, (batch_size, sequence_length), device=device)\n",
    "\n",
    "        # Use the generator to create fake sequences\n",
    "        fake_sequences = generator(noise_sequences)\n",
    "\n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_output = discriminator(real_sequences)\n",
    "        fake_output = discriminator(fake_sequences.detach())\n",
    "        loss_D_real = loss_function(real_output, torch.ones_like(real_output))\n",
    "        loss_D_fake = loss_function(fake_output, torch.zeros_like(fake_output))\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_output = discriminator(fake_sequences)\n",
    "        loss_G = loss_function(fake_output, torch.ones_like(fake_output))\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_loss_D = 0\n",
    "        val_loss_G = 0\n",
    "        for real_sequences in val_loader:\n",
    "            real_sequences = real_sequences.to(device)\n",
    "            noise_sequences = torch.randint(vocab_size, (batch_size, sequence_length), device=device)\n",
    "            fake_sequences = generator(noise_sequences)\n",
    "            real_output = discriminator(real_sequences)\n",
    "            fake_output = discriminator(fake_sequences)\n",
    "            val_loss_D += (loss_function(real_output, torch.ones_like(real_output)) + loss_function(fake_output, torch.zeros_like(fake_output))) / 2\n",
    "            val_loss_G += loss_function(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss D: {loss_D.item()}, Loss G: {loss_G.item()}, Val Loss D: {val_loss_D / len(val_loader)}, Val Loss G: {val_loss_G / len(val_loader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END GAN SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your seed sequence\n",
    "seed_sequence = ['[resourceType]CarePlan']\n",
    "#CarePlan,\"[resourceType] CarePlan [id] 5026185d-3747-e969-6564-c4aec1b8d06e [meta][profile][0] http://hl7.org/fhir/us/core/StructureDefinition/us-core-careplan [text][status] generated [text][div] <div xmlns=\"\"http://www.w3.org/1999/xhtml\"\">Care Plan for Infectious disease care plan (record artifact).<br/>Activities: <ul><li>Infectious disease care plan (record artifact)</li><li>Infectious disease care plan (record artifact)</li><li>Infectious disease care plan (record artifact)</li></ul><br/>Care plan is meant to treat Suspected COVID-19.</div> [status] completed [intent] order [category][0][coding][0][system] http://hl7.org/fhir/us/core/CodeSystem/careplan-category [category][0][coding][0][code] assess-plan [category][1][coding][0][system] http://snomed.info/sct [category][1][coding][0][code] 736376001 [category][1][coding][0][display] Infectious disease care plan (record artifact) [category][1][text] Infectious disease care plan (record artifact) [subject][reference] Patient/2b7d4554-d6e4-9f48-2ab0-0ddf088fe19d [encounter][reference] Encounter/723aa866-47af-c57d-fe71-cebfba9b15cf [period][start] 2020-02-28T22:59:13-05:00 [period][end] 2020-03-14T02:15:17-04:00 [careTeam][0][reference] CareTeam/49500403-2881-84d8-cb65-015bfea4c65d [addresses][0][reference] Condition/c925509e-0334-6544-6af4-e6ffa96144b3 [activity][0][detail][code][coding][0][system] http://snomed.info/sct [activity][0][detail][code][coding][0][code] 444908001 [activity][0][detail][code][coding][0][display] Isolation nursing in negative pressure isolation environment (regime/therapy) [activity][0][detail][code][text] Isolation nursing in negative pressure isolation environment (regime/therapy) [activity][0][detail][status] completed [activity][0][detail][location][display] NEWTON-WELLESLEY HOSPITAL [activity][1][detail][code][coding][0][system] http://snomed.info/sct [activity][1][detail][code][coding][0][code] 409524006 [activity][1][detail][code][coding][0][display] Airborne precautions (procedure) [activity][1][detail][code][text] Airborne precautions (procedure) [activity][1][detail][status] completed [activity][1][detail][location][display] NEWTON-WELLESLEY HOSPITAL [activity][2][detail][code][coding][0][system] http://snomed.info/sct [activity][2][detail][code][coding][0][code] 409526008 [activity][2][detail][code][coding][0][display] Personal protective equipment (physical object) [activity][2][detail][code][text] Personal protective equipment (physical object) [activity][2][detail][status] completed [activity][2][detail][location][display] NEWTON-WELLESLEY HOSPITAL\"\n",
    "# Convert the seed sequence to a tensor of indices\n",
    "seed_tensor = torch.tensor([vocab[token] for token in seed_sequence]).unsqueeze(0).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the generated sequence with the seed sequence\n",
    "generated_sequence = seed_sequence\n",
    "\n",
    "# Generate a sequence of length 100\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "        # Get the model's prediction for the next token\n",
    "        output = model(seed_tensor)\n",
    "\n",
    "        # Get the index of the predicted token\n",
    "        predicted_index = output.argmax(dim=-1)[0, -1].item()\n",
    "\n",
    "        # Convert the index to a token\n",
    "        predicted_token = vocab.get_itos()[predicted_index]\n",
    "\n",
    "        # Add the predicted token to the generated sequence\n",
    "        generated_sequence.append(predicted_token)\n",
    "\n",
    "        # Add the predicted index to the seed tensor\n",
    "        seed_tensor = torch.cat([seed_tensor, torch.tensor([[predicted_index]]).to(device)], dim=1)\n",
    "\n",
    "# Print the generated sequence\n",
    "print(' '.join(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def string_to_dict(s):\n",
    "    split_list = re.findall('\\[.*?\\]\\s[^\\[]*|\\[.*?\\]\\s|\\s', s)\n",
    "    result = {}\n",
    "\n",
    "    for item in split_list:\n",
    "        key, value = item.split('] ')\n",
    "        keys = key.replace(\"][\", \"|\").strip(\"[\").split(\"|\")\n",
    "        temp = result\n",
    "        for k in keys[:-1]:\n",
    "            if not k.isdigit():\n",
    "                if k not in temp:\n",
    "                    # If the next key is a number, then this key should be a list\n",
    "                    if keys[keys.index(k) + 1].isdigit():\n",
    "                        temp[k] = []\n",
    "                    else:\n",
    "                        temp[k] = {}\n",
    "                temp = temp[k]\n",
    "            else:\n",
    "                if len(temp) <= int(k):\n",
    "                    temp.append({})  # append a new dictionary to the list\n",
    "                temp = temp[int(k)]  # Go to the dictionary at index k in the list\n",
    "\n",
    "        if isinstance(temp, list):\n",
    "            if len(temp) <= int(keys[-1]):\n",
    "                temp.append(value.strip())  # append the new value and strip whitespace\n",
    "            else:  \n",
    "                temp[int(keys[-1])] = value.strip()  # if the index already exists, replace the value and strip whitespace\n",
    "        else:\n",
    "            temp[keys[-1]] = value.strip()  # strip whitespace\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def dict_to_json(dict_obj):\n",
    "    return json.dumps(dict_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhir_dict = string_to_dict(generated_sequence)\n",
    "json_string = dict_to_json(fhir_dict)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cardinality(value, min, max):\n",
    "    \"\"\"\n",
    "    Check that the cardinality of the value is within the allowed range.\n",
    "    \"\"\"\n",
    "    # Convert '*' to a large number\n",
    "    max = float('inf') if max == '*' else int(max)\n",
    "\n",
    "    # If the value is a list, check the length of the list\n",
    "    if isinstance(value, list):\n",
    "        length = len(value)\n",
    "    else:\n",
    "        # Otherwise, consider it as a single value\n",
    "        length = 1\n",
    "\n",
    "    if length < int(min) or length > max:\n",
    "        raise ValueError(f\"Cardinality {length} not in range {min}..{max}\")\n",
    "\n",
    "def check_type(value, expected_type):\n",
    "    \"\"\"\n",
    "    Check that the value is of the correct type.\n",
    "    \"\"\"\n",
    "    # Get the actual type of the value\n",
    "    actual_type = type(value).__name__\n",
    "\n",
    "    # The expected type is a list of dictionaries, extract the code field\n",
    "    expected_type_codes = [t['code'] for t in expected_type]\n",
    "\n",
    "    # Map the FHIR data types to Python data types\n",
    "    type_mapping = {\n",
    "        'boolean': 'bool',\n",
    "        'integer': 'int',\n",
    "        'string': 'str',\n",
    "        'decimal': 'float',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "\n",
    "    expected_python_types = [type_mapping[code] for code in expected_type_codes if code in type_mapping]\n",
    "\n",
    "    if actual_type not in expected_python_types:\n",
    "        raise ValueError(f\"Type {actual_type} not in {expected_python_types}\")\n",
    "      \n",
    "def validate_resource(resource, structure_definitions):\n",
    "    # Identify the resource type\n",
    "    resource_type = resource['resourceType']\n",
    "\n",
    "    # Find the corresponding StructureDefinition\n",
    "    structure_definition = next(sd for sd in structure_definitions if sd['type'] == resource_type)\n",
    "\n",
    "    # Check each field in the resource\n",
    "    for field, value in resource.items():\n",
    "        if field not in structure_definition['element']:\n",
    "            raise ValueError(f\"Unexpected field {field}\")\n",
    "        element_definition = structure_definition['element'][field]\n",
    "        check_cardinality(value, element_definition['min'], element_definition['max'])\n",
    "        check_type(value, element_definition['type'])\n",
    "\n",
    "    # Check for missing fields\n",
    "    for field, element_definition in structure_definition['element'].items():\n",
    "        if element_definition['min'] > 0 and field not in resource:\n",
    "            raise ValueError(f\"Missing required field {field}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_resource(json_string, 'CarePlan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
