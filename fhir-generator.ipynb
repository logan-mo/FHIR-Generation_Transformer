{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References <br>\n",
    "https://medium.com/towards-data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021 <br>\n",
    "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=14TR8qzYX9QQPxpoRr779axbmpp0REGeP'\n",
    "output = 'fhir-chunk-1.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch dataset class\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a DataSet Class that returns a FHIR resource as a PyTorch DataSet\n",
    "\n",
    "class FHIRData(Dataset):\n",
    "    def __init__(self, data_frame_path):\n",
    "        if os.path.isfile(data_frame_path):\n",
    "            self.data_frame = pd.read_csv(data_frame_path)\n",
    "        elif os.path.isdir(data_frame_path):\n",
    "            self.data_frame = pd.concat([pd.read_csv(os.path.join(data_frame_path, file)) for file in os.listdir(data_frame_path)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        return row['seed'], row['FHIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FHIRDataSet('data\\\\time_seed_fhir.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(tokenizer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens(list_of_new_tokens)\n",
    "\n",
    "# # resize the embeddings\n",
    "#  model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# https://discuss.huggingface.co/t/t5-for-conditional-generation-getting-started/1284/8\n",
    "\n",
    "# https://discuss.huggingface.co/t/t5-finetuning-tips/684\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "n_train = 64\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (n_train, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (n_train, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Fine Tuning Pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import BartModel, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"\"\n",
    "# If colab\n",
    "# base_dir = '/content/'\n",
    "data_path = base_dir + 'data/time_seed_fhir.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhir_dataset = load_dataset('csv', data_files=data_path, split='train')\n",
    "fhir_dataset = fhir_dataset.train_test_split(test_size=0.2)\n",
    "print(f\"Training Samples: {fhir_dataset['train'].num_rows}, Test Samples: {fhir_dataset['test'].num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Generate a FHIR data row using the seed: \"\n",
    "# FHIR,seed\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"seed\"]]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"longest\")\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"FHIR\"], truncation=True, padding=\"longest\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = fhir_dataset.map(preprocess_function, batched=True).remove_columns(['seed', 'FHIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartModel.from_pretrained('facebook/bart-large', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir                  = \"./results\",\n",
    "    evaluation_strategy         = \"epoch\",\n",
    "    learning_rate               = 2e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size  = 32,\n",
    "    weight_decay                = 0.01,\n",
    "    save_total_limit            = 3,\n",
    "    num_train_epochs            = 1,\n",
    "    fp16                        = True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset  = tokenized_dataset['test'],\n",
    "    tokenizer     = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Fine Tuning Pre-trained T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n",
    "\n",
    "# https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"\"\n",
    "# If colab\n",
    "# base_dir = '/content/'\n",
    "data_path = base_dir + 'data/time_seed_fhir.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhir_dataset = load_dataset('csv', data_files=data_path, split='train')\n",
    "fhir_dataset = fhir_dataset.train_test_split(test_size=0.2)\n",
    "print(f\"Training Samples: {fhir_dataset['train'].num_rows}, Test Samples: {fhir_dataset['test'].num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Generate a FHIR data row using the seed: \"\n",
    "# FHIR,seed\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"seed\"]]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"longest\")\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"FHIR\"], truncation=True, padding=\"longest\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = fhir_dataset.map(preprocess_function, batched=True).remove_columns(['seed', 'FHIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\")\n",
    "\n",
    "# input_text = \"[resourceType] CarePlan [id] 76d3b4b2-398f-4373-4957-599959c26e87 [meta][profile][0] http://hl7.org/fhir/us/core/StructureDefinition/us-core-careplan [text][status] generated [text][div] <div xmlns=\"\"http://www.w3.org/1999/xhtml\"\">Care Plan for Infectious disease care plan (record artifact).<br/>Activities: <ul><li>Infectious disease care plan (record artifact)</li><li>Infectious disease care plan (record artifact)</li></ul><br/>Care plan is meant to treat COVID-19.</div> [status] completed [intent] order [category][0][coding][0][system] http://hl7.org/fhir/us/core/CodeSystem/careplan-category [category][0][coding][0][code] assess-plan [category][1][coding][0][system] http://snomed.info/sct [category][1][coding][0][code] 736376001 [category][1][coding][0][display] Infectious disease care plan (record artifact) [category][1][text] Infectious disease care plan (record artifact) [subject][reference] Patient/4a62b5fe-0dbd-fef9-e9a9-c21cecc5df61 [encounter][reference] Encounter/3bffc09e-9247-6538-e8db-6c119e3f1e2c [period][start] 2020-03-09T16:39:52-04:00 [period][end] 2020-03-27T13:39:52-04:00 [careTeam][0][reference] CareTeam/51954d07-f4a7-0f1a-b7a3-9c0f1f592038 [addresses][0][reference] Condition/3e78ad09-6fec-ede1-b27a-47e79bee28f1 [activity][0][detail][code][coding][0][system] http://snomed.info/sct [activity][0][detail][code][coding][0][code] 409524006 [activity][0][detail][code][coding][0][display] Airborne precautions (procedure) [activity][0][detail][code][text] Airborne precautions (procedure) [activity][0][detail][status] completed [activity][0][detail][location][display] BERKSHIRE MEDICAL CENTER INC - 1 [activity][1][detail][code][coding][0][system] http://snomed.info/sct [activity][1][detail][code][coding][0][code] 361235007 [activity][1][detail][code][coding][0][display] Isolation of infected patient (procedure) [activity][1][detail][code][text] Isolation of infected patient (procedure) [activity][1][detail][status] completed [activity][1][detail][location][display] BERKSHIRE MEDICAL CENTER INC - 1\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir                  = \"./results\",\n",
    "    evaluation_strategy         = \"epoch\",\n",
    "    learning_rate               = 2e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size  = 32,\n",
    "    weight_decay                = 0.01,\n",
    "    save_total_limit            = 3,\n",
    "    num_train_epochs            = 1,\n",
    "    fp16                        = True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    eval_dataset  = tokenized_dataset['test'],\n",
    "    tokenizer     = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Messing with a locally deployed LLM - 1 (Flan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Messing with a locally deployed LLM - 2 (llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
